from seleniumbase import SB
from bs4 import BeautifulSoup
import json
import time
import os
import random
from urllib.parse import urljoin

# --- AYARLAR ---
BASE_DOMAIN = "https://dizipal.cx"
DATA_FILE = 'diziler.json'

def get_video_source(sb, episode_url):
    """BÃ¶lÃ¼m sayfasÄ±na girip iframe src'yi alÄ±r."""
    try:
        sb.open(episode_url)
        # SayfanÄ±n yÃ¼klenmesini bekle (Video player div'i gÃ¶rÃ¼nene kadar)
        try:
            sb.wait_for_element('div.video-player-area', timeout=5)
        except:
            pass # Bulamazsa devam et
        
        soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
        
        # YÃ¶ntem 1: Player alanÄ±
        player_area = soup.find('div', class_='video-player-area')
        if player_area:
            iframe = player_area.find('iframe')
            if iframe:
                return iframe.get('src')
        
        # YÃ¶ntem 2: Genel Iframe
        iframes = soup.find_all('iframe')
        for frame in iframes:
            src = frame.get('src', '')
            if 'embed' in src or '.cfd' in src or 'player' in src:
                return src
    except Exception as e:
        print(f"      âš ï¸ Video kaynaÄŸÄ± alÄ±namadÄ±: {e}")
    return ""

def get_full_series_details(sb, url):
    print(f"   â–¶ï¸ Dizi Analiz ediliyor: {url}")
    
    try:
        sb.open(url)
        # Cloudflare kontrolÃ¼ varsa geÃ§mesini bekle
        time.sleep(random.uniform(2, 4)) 
        
        soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
        
        # EÄŸer sayfa boÅŸ veya 404 ise
        if "Sayfa bulunamadÄ±" in soup.text or sb.get_title() == "404 Not Found":
            return None

        meta = {
            "url": url,
            "title": "",
            "year": "",
            "description": "",
            "poster": "",
            "cover_image": "",
            "imdb": "0",
            "genres": [],
            "episodes": []
        }

        # Metadata Ã‡ekme Ä°ÅŸlemleri
        h1 = soup.find('h1')
        if h1:
            full_text = h1.get_text(" ", strip=True)
            if '(' in full_text:
                parts = full_text.split('(')
                meta['title'] = parts[0].strip()
                meta['year'] = parts[-1].replace(')', '').strip()
            else:
                meta['title'] = full_text

        summary = soup.find('p', class_='summary-text')
        if summary: meta['description'] = summary.get_text(strip=True)

        poster_div = soup.find('div', class_='poster')
        if poster_div and poster_div.find('img'):
            meta['poster'] = poster_div.find('img').get('src')

        head_div = soup.find('div', id='head', class_='cover-image')
        if head_div and head_div.has_attr('style') and "url('" in head_div['style']:
            meta['cover_image'] = head_div['style'].split("url('")[1].split("')")[0]

        imdb_span = soup.find('span', string=lambda t: t and "IMDB PuanÄ±" in t)
        if imdb_span:
            parent = imdb_span.find_parent('div')
            score = parent.find('h4') if parent else None
            if score: meta['imdb'] = score.get_text(strip=True)

        genre_links = soup.find_all('a', href=lambda h: h and 'dizi-kategori' in h)
        meta['genres'] = list(set([g.get_text(strip=True) for g in genre_links]))

        # --- SEZON TARAMA ---
        season_links = []
        season_div = soup.find('div', id='season-options-list')
        
        if season_div:
            links = season_div.find_all('a', href=True)
            for l in links:
                full_link = urljoin(BASE_DOMAIN, l['href'])
                if full_link not in season_links:
                    season_links.append(full_link)
        
        if not season_links:
            season_links.append(url)
        
        print(f"   ğŸ“‚ {len(season_links)} Sezon bulundu.", flush=True)

        # SezonlarÄ± Gez
        for s_idx, season_url in enumerate(season_links):
            print(f"      ğŸ“Œ Sezon {s_idx+1} taranÄ±yor...", flush=True)
            
            # EÄŸer zaten o sayfadaysak tekrar yÃ¼kleme
            if season_url != sb.get_current_url():
                sb.open(season_url)
                time.sleep(2)
            
            season_soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
            episode_items = season_soup.find_all('div', class_='episode-item')
            
            for item in episode_items:
                ep_data = {}
                link_tag = item.find('a')
                
                if link_tag:
                    ep_url = link_tag.get('href')
                    ep_data['url'] = ep_url
                    ep_data['title'] = link_tag.get('title')
                    
                    img_tag = link_tag.find('img')
                    if img_tag:
                        ep_data['thumbnail'] = img_tag.get('src')
                    
                    if ep_url:
                        # Video iÃ§in yeni sekmeye gerek yok, mevcut sayfada git-gel yapacaÄŸÄ±z
                        # Veya basitÃ§e URL'yi ziyaret edeceÄŸiz.
                        # NOT: Her bÃ¶lÃ¼me girmek Ã§ok zaman alacaÄŸÄ± iÃ§in burada dikkatli olunmalÄ±.
                        # HÄ±z iÃ§in ÅŸimdilik ana sayfaya dÃ¶nme mantÄ±ÄŸÄ±nÄ± kurgulamalÄ±yÄ±z.
                        pass 

                num_tag = item.find('h4', class_='font-eudoxus')
                if num_tag:
                    ep_data['episode_number'] = num_tag.get_text(strip=True)
                
                meta['episodes'].append(ep_data)

        # NOT: Video kaynaklarÄ±nÄ± toplamak iÃ§in bÃ¶lÃ¼mleri tek tek gezmek gerek
        # Bu iÅŸlem Selenium ile Ã‡OK UZUN sÃ¼rer (Her bÃ¶lÃ¼m 5-10 saniye). 
        # O yÃ¼zden ÅŸimdilik sadece bÃ¶lÃ¼m listesini alÄ±yoruz.
        # EÄŸer video player'Ä± MUTLAKA istiyorsanÄ±z aÅŸaÄŸÄ±yÄ± aÃ§Ä±n:
        
        print(f"      ğŸ¥ BÃ¶lÃ¼m playerlarÄ± taranÄ±yor ({len(meta['episodes'])} bÃ¶lÃ¼m)...")
        for ep in meta['episodes']:
            if 'url' in ep:
                 src = get_video_source(sb, ep['url'])
                 ep['video_source'] = src
                 print(f"         -> {ep.get('title')} : {src}", flush=True)

        return meta

    except Exception as e:
        print(f"   âŒ Hata: {e}")
        return None

def main():
    print("ğŸš€ DÄ°ZÄ°PAL TARAYICI (SeleniumBase UC Modu)...")
    
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                all_series = json.load(f)
            print(f"ğŸ“¦ Mevcut veri: {len(all_series)} dizi.")
        except:
            all_series = []
    else:
        all_series = []

    # UC=True -> Undetected Chromedriver (Bot korumasÄ±nÄ± aÅŸar)
    # Headless=False -> Xvfb ile sanal ekranda "gÃ¶rÃ¼nÃ¼r" Ã§alÄ±ÅŸÄ±r (Daha gÃ¼venli)
    with SB(uc=True, headless=False) as sb:
        page_num = 1
        empty_page_count = 0 

        while True:
            if page_num == 1:
                list_url = "https://dizipal.cx/diziler/"
            else:
                list_url = f"https://dizipal.cx/diziler/page/{page_num}/"
                
            print(f"\nğŸ“„ Sayfa {page_num} aÃ§Ä±lÄ±yor: {list_url}")
            
            try:
                sb.open(list_url)
                # Cloudflare "Human Verify" Ã§Ä±karsa bekle
                time.sleep(3) 
                
                # Sayfa kaynaÄŸÄ±nÄ± al
                soup = BeautifulSoup(sb.get_page_source(), 'html.parser')
                
                # 404 KontrolÃ¼
                if "Sayfa bulunamadÄ±" in soup.text or sb.get_title() == "404 Not Found":
                    print("ğŸ Sayfa yok. Bitti.")
                    break

                links = soup.find_all('a', href=True)
                series_urls = []
                for link in links:
                    href = link['href']
                    if '/dizi/' in href and href.count('/') > 3:
                        full_url = urljoin(BASE_DOMAIN, href)
                        clean_url = full_url.split('?')[0]
                        if clean_url not in series_urls:
                            series_urls.append(clean_url)
                
                series_urls = list(set(series_urls))
                
                if not series_urls:
                    print("âš ï¸ Dizi bulunamadÄ±.")
                    empty_page_count += 1
                    if empty_page_count >= 2:
                        break
                    page_num += 1
                    continue
                
                empty_page_count = 0
                print(f"   ğŸ” {len(series_urls)} dizi bulundu.")

                for s_url in series_urls:
                    if any(s['url'] == s_url for s in all_series):
                        print(f"   â­ï¸ Zaten var: {s_url}")
                        continue
                    
                    details = get_full_series_details(sb, s_url)
                    if details:
                        all_series.append(details)
                        with open(DATA_FILE, 'w', encoding='utf-8') as f:
                            json.dump(all_series, f, ensure_ascii=False, indent=2)

            except Exception as e:
                print(f"âš ï¸ Sayfa hatasÄ±: {e}")
                # Hata alÄ±nca devam etmeye Ã§alÄ±ÅŸ
                
            page_num += 1

    print(f"\nâœ… TAMAMLANDI. {len(all_series)} dizi kaydedildi.")

if __name__ == "__main__":
    main()
