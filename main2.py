import requests
from bs4 import BeautifulSoup
import json
import time
import os

# --- AYARLAR ---
BASE_DOMAIN = "https://dizipal.cx"
BASE_URL = "https://dizipal.cx/diziler/page/{}/"  # Sayfalama yapÄ±sÄ±
DATA_FILE = 'diziler.json'
MAX_RETRIES = 3

# TarayÄ±cÄ± gibi gÃ¶rÃ¼nmek iÃ§in Header
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': f'{BASE_DOMAIN}/',
}

def get_soup(url, retry_count=0):
    """Verilen URL'e istek atÄ±p BeautifulSoup objesi dÃ¶ner."""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
        elif response.status_code == 404:
            return "404"
    except Exception as e:
        if retry_count < MAX_RETRIES:
            time.sleep(2)
            return get_soup(url, retry_count + 1)
    return None

def get_video_source(episode_url):
    """
    BÃ¶lÃ¼m sayfasÄ±na gider ve iframe iÃ§indeki video linkini Ã§eker.
    """
    soup = get_soup(episode_url)
    if not soup or soup == "404":
        return ""
    
    try:
        # 1. YÃ¶ntem: Senin gÃ¶nderdiÄŸin HTML yapÄ±sÄ±ndaki player alanÄ±
        player_area = soup.find('div', class_='video-player-area')
        if player_area:
            iframe = player_area.find('iframe')
            if iframe:
                return iframe.get('src')
        
        # 2. YÃ¶ntem: Yedek (Fallback) - Sayfadaki herhangi bir embed iframe'i bul
        iframes = soup.find_all('iframe')
        for frame in iframes:
            src = frame.get('src', '')
            # Genelde video linkleri bu kelimeleri veya uzantÄ±larÄ± iÃ§erir
            if 'embed' in src or '.cfd' in src or 'player' in src or 'youtube' in src:
                return src
                
    except Exception as e:
        print(f"      âš ï¸ Player Ã§ekilemedi: {e}")
    
    return ""

def get_full_series_details(url):
    """
    Dizi detay sayfasÄ±nÄ± ve bÃ¶lÃ¼mleri tarar.
    """
    soup = get_soup(url)
    if not soup or soup == "404":
        return None
    
    meta = {
        "title": "",
        "year": "",
        "description": "",
        "poster": "",
        "cover_image": "",
        "imdb": "0",
        "genres": [],
        "episodes": []
    }
    
    try:
        # --- 1. BaÅŸlÄ±k ve YÄ±l ---
        h1 = soup.find('h1')
        if h1:
            full_text = h1.get_text(" ", strip=True)
            if '(' in full_text:
                parts = full_text.split('(')
                meta['title'] = parts[0].strip()
                meta['year'] = parts[-1].replace(')', '').strip()
            else:
                meta['title'] = full_text

        # --- 2. AÃ§Ä±klama ---
        summary = soup.find('p', class_='summary-text')
        if summary:
            meta['description'] = summary.get_text(strip=True)

        # --- 3. Poster ---
        poster_div = soup.find('div', class_='poster')
        if poster_div and poster_div.find('img'):
            meta['poster'] = poster_div.find('img').get('src')

        # --- 4. Kapak Resmi (Background) ---
        head_div = soup.find('div', id='head', class_='cover-image')
        if head_div and head_div.has_attr('style'):
            style = head_div['style']
            if "url('" in style:
                meta['cover_image'] = style.split("url('")[1].split("')")[0]

        # --- 5. IMDB PuanÄ± ---
        imdb_span = soup.find('span', string=lambda t: t and "IMDB PuanÄ±" in t)
        if imdb_span:
            parent = imdb_span.find_parent('div')
            score = parent.find('h4') if parent else None
            if score:
                meta['imdb'] = score.get_text(strip=True)

        # --- 6. TÃ¼rler ---
        genre_links = soup.find_all('a', href=lambda h: h and 'dizi-kategori' in h)
        meta['genres'] = list(set([g.get_text(strip=True) for g in genre_links]))

        # --- 7. BÃ¶lÃ¼mler ve Player ---
        episode_items = soup.find_all('div', class_='episode-item')
        print(f"   ğŸ“‚ {len(episode_items)} bÃ¶lÃ¼m bulundu. Player linkleri taranÄ±yor...", flush=True)

        for item in episode_items:
            ep_data = {}
            link_tag = item.find('a')
            
            if link_tag:
                ep_url = link_tag.get('href')
                ep_data['url'] = ep_url
                ep_data['title'] = link_tag.get('title')
                
                # Thumbnail
                img_tag = link_tag.find('img')
                if img_tag:
                    ep_data['thumbnail'] = img_tag.get('src')
                
                # --- VÄ°DEO KAYNAÄINI Ã‡EKME ---
                if ep_url:
                    # Sunucuyu yormamak iÃ§in Ã§ok kÄ±sa bekleme
                    # time.sleep(0.5) 
                    video_src = get_video_source(ep_url)
                    ep_data['video_source'] = video_src
                    print(f"      âœ… {ep_data.get('title', 'BÃ¶lÃ¼m')} -> Kaynak AlÄ±ndÄ±", flush=True)
            
            # BÃ¶lÃ¼m numarasÄ±
            num_tag = item.find('h4', class_='font-eudoxus')
            if num_tag:
                ep_data['episode_number'] = num_tag.get_text(strip=True)
            
            meta['episodes'].append(ep_data)

    except Exception as e:
        print(f"   âŒ Detay hatasÄ±: {e}")

    return meta

def main():
    print("ğŸš€ Dizi TarayÄ±cÄ± BaÅŸlatÄ±lÄ±yor...")
    
    # Mevcut veriyi yÃ¼kle
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            all_series = json.load(f)
        print(f"ğŸ“¦ Mevcut {len(all_series)} dizi yÃ¼klendi.")
    else:
        all_series = []

    page_num = 1
    consecutive_errors = 0

    while True:
        url = BASE_URL.format(page_num)
        print(f"\nğŸ“„ Sayfa {page_num} taranÄ±yor: {url}")
        
        soup = get_soup(url)
        
        if soup == "404":
            print("ğŸ Sayfa bulunamadÄ± (404). Tarama bitti.")
            break
        
        if not soup:
            consecutive_errors += 1
            if consecutive_errors > 3:
                print("âš ï¸ Ã‡ok fazla hata alÄ±ndÄ±, Ã§Ä±kÄ±lÄ±yor.")
                break
            continue
            
        consecutive_errors = 0
        
        # Sayfadaki dizi kartlarÄ±nÄ± bul (Wordpress yapÄ±sÄ±na gÃ¶re genelde article veya post-item olur)
        # Dizipal HTML yapÄ±sÄ±na gÃ¶re kartlarÄ± buluyoruz.
        # Genelde linkler 'dizi/dizi-adi' formatÄ±ndadÄ±r.
        links = soup.find_all('a', href=True)
        series_links = []
        
        for link in links:
            href = link['href']
            # Sadece dizi detay sayfalarÄ±nÄ± al, pagination veya kategorileri alma
            if '/dizi/' in href and href.count('/') > 4: # Basit bir filtreleme
                if href not in series_links:
                    series_links.append(href)

        # MÃ¼kerrerleri temizle
        series_links = list(set(series_links))
        
        if not series_links:
            print("âš ï¸ Bu sayfada dizi bulunamadÄ±. Sonraki sayfaya geÃ§iliyor.")
            # Sayfa boÅŸsa ama 404 deÄŸilse dÃ¶ngÃ¼ye devam etmesi iÃ§in
            # page_num += 1; continue; diyebiliriz ama sonsuz dÃ¶ngÃ¼ riski var.
            # Åimdilik devam ediyoruz.

        print(f"   ğŸ” Bu sayfada {len(series_links)} dizi bulundu.")

        for s_url in series_links:
            # Zaten ekli mi kontrol et
            if any(s['url'] == s_url for s in all_series if 'url' in s):
                print(f"   â­ï¸ Pas geÃ§ildi (Zaten var): {s_url}")
                continue
            
            print(f"   â–¶ï¸ Ä°ÅŸleniyor: {s_url}")
            details = get_full_series_details(s_url)
            
            if details:
                details['url'] = s_url # KayÄ±t iÃ§in URL'i de ekleyelim
                all_series.append(details)
                
                # Her diziden sonra kaydet (Veri kaybÄ±nÄ± Ã¶nlemek iÃ§in)
                with open(DATA_FILE, 'w', encoding='utf-8') as f:
                    json.dump(all_series, f, ensure_ascii=False, indent=2)

        page_num += 1
        # time.sleep(1) # Sayfalar arasÄ± bekleme

if __name__ == "__main__":
    main()
