import requests
from bs4 import BeautifulSoup
import json
import time
import os

# --- AYARLAR ---
BASE_DOMAIN = "https://dizipal.cx"
DATA_FILE = 'movies.json'
MAX_RETRIES = 3 # Bir sayfa hata verirse kaÃ§ kez denenecek
FAILED_THRESHOLD = 3 # Ãœst Ã¼ste kaÃ§ sayfa hata verirse duracak

def get_soup(url, retry_count=0):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': f'{BASE_DOMAIN}/',
    }
    try:
        response = requests.get(url, headers=headers, timeout=20)
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
        elif response.status_code == 404:
            return "404" # Sayfa gerÃ§ekten yok
        else:
            raise Exception(f"Status Code: {response.status_code}")
    except Exception as e:
        if retry_count < MAX_RETRIES:
            print(f"   âš ï¸ Hata: {url}. {retry_count+1}. deneme yapÄ±lÄ±yor...", flush=True)
            time.sleep(2)
            return get_soup(url, retry_count + 1)
        return None

def get_full_movie_details(url):
    soup = get_soup(url)
    details = {"videoUrl": url, "description": "AÃ§Ä±klama bulunamadÄ±.", "imdb": "0.0", "genres": [], "cast": [], "year": ""}
    if not soup or soup == "404": return details
    try:
        iframe = soup.find('iframe')
        if iframe and 'src' in iframe.attrs: details["videoUrl"] = iframe['src']
        summary_title = soup.find('h6', string='Film Ã–zeti')
        if summary_title:
            summary_p = summary_title.find_next('p')
            if summary_p: details["description"] = summary_p.get_text(strip=True)
        # Detay kutularÄ±nÄ± Ã§ekme...
        info_boxes = soup.find_all('div', class_=lambda x: x and 'rounded-[10px]' in x and 'bg-white/[4%]' in x)
        for box in info_boxes:
            label_span = box.find('span', class_='text-xs')
            if label_span:
                label = label_span.get_text(strip=True)
                val_div = label_span.find_next_sibling('div') or label_span.find_next_sibling('h6')
                if val_div:
                    if "IMDB PuanÄ±" in label: details["imdb"] = val_div.get_text(strip=True)
                    elif "TÃ¼r" in label: details["genres"] = [a.get_text(strip=True) for a in val_div.find_all('a')]
                    elif "Oyuncular" in label: details["cast"] = [a.get_text(strip=True) for a in val_div.find_all('a')]
                    elif "YapÄ±m YÄ±lÄ±" in label: details["year"] = val_div.get_text(strip=True)
    except: pass
    return details

def start_scraping():
    all_films = []
    page_num = 1
    consecutive_failed_pages = 0
    
    print(f"ğŸ›¡ï¸ GÃ¼neÅŸ TV: YÃ¼ksek ToleranslÄ± Tarama BaÅŸlatÄ±ldÄ±...", flush=True)

    while consecutive_failed_pages < FAILED_THRESHOLD:
        target_url = f"{BASE_DOMAIN}/filmler/page/{page_num}/"
        print(f"\n--- ğŸ“„ SAYFA {page_num} ANALÄ°ZÄ° ---", flush=True)
        
        soup = get_soup(target_url)
        
        if soup is None or soup == "404":
            consecutive_failed_pages += 1
            print(f"   âŒ Sayfa alÄ±namadÄ± ({consecutive_failed_pages}/{FAILED_THRESHOLD})", flush=True)
            page_num += 1
            continue

        # EÄŸer buraya geldiysek sayfa baÅŸarÄ±lÄ±dÄ±r, hata sayacÄ±nÄ± sÄ±fÄ±rla
        consecutive_failed_pages = 0
        items = soup.find_all('div', class_='post-item')
        
        if not items:
            print("   ğŸš« Film listesi boÅŸ. ArÅŸiv bitti.", flush=True)
            break

        print(f"   ğŸ“¦ {len(items)} film bulundu.", flush=True)

        for item in items:
            try:
                link_element = item.find('a')
                if not link_element: continue
                title = link_element.get('title', '').strip()
                movie_url = link_element.get('href', '')
                img_element = item.find('img')
                image = img_element.get('data-src') or img_element.get('src') or ""
                
                meta = get_full_movie_details(movie_url)
                all_films.append({
                    'title': title, 'image': image, 'imdb': meta["imdb"], 
                    'year': meta["year"], 'genres': meta["genres"], 
                    'cast': meta["cast"], 'description': meta["description"], 
                    'videoUrl': meta["videoUrl"]
                })
                print(f"   âœ… [{len(all_films)}] {title}", flush=True)
            except: continue

        # SayfanÄ±n en altÄ±nda "Next/Sonraki" butonu var mÄ± kontrolÃ¼
        # Bu, sona geldiÄŸimizi anlamanÄ±n en kesin yoludur.
        pagination = soup.find('div', class_='pagination') or soup.find('div', class_='nav-links')
        if pagination:
            next_button = pagination.find('a', class_=lambda x: x and ('next' in x or 'next-page' in x))
            if not next_button and page_num > 5: # Ä°lk sayfalarda deÄŸilsek ve next yoksa bitmiÅŸtir
                print("   ğŸ Sonraki sayfa butonu bulunamadÄ±. ArÅŸiv tamamlandÄ±.", flush=True)
                # break # Ä°stersen burada da kÄ±rabilirsin ama ardÄ±ÅŸÄ±k hata kontrolÃ¼ daha garanti.

        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(all_films, f, ensure_ascii=False, indent=2)
        
        page_num += 1

    print(f"\nğŸ‰ Ä°ÅŸlem tamamlandÄ±. Toplam veri: {len(all_films)}", flush=True)

if __name__ == "__main__":
    start_scraping()
