from seleniumbase import SB
from curl_cffi import requests
from bs4 import BeautifulSoup
import json
import time
import os
import random
from urllib.parse import urljoin

# --- AYARLAR ---
BASE_DOMAIN = "https://dizipal.cx"
DATA_FILE = 'movies.json'
CHECK_LIMIT = 30  # KaÃ§ tane 'zaten var' olan film gÃ¶rÃ¼nce dursun?

# Global Session (HÄ±z iÃ§in)
session = requests.Session()

def get_cookies_and_ua_with_selenium():
    """Selenium ile siteye girip Cloudflare Ã§erezlerini ve User-Agent'Ä± alÄ±r."""
    print("ğŸ”“ Selenium ile Cloudflare kilidi aÃ§Ä±lÄ±yor (Filmler)...", flush=True)
    cookies = {}
    user_agent = ""
    
    with SB(uc=True, headless=False) as sb:
        try:
            # Filmler sayfasÄ±na gidiyoruz
            sb.open(BASE_DOMAIN + "/filmler/")
            time.sleep(6) # Cloudflare kontrolÃ¼ iÃ§in bekleme
            
            title = sb.get_title()
            print(f"   ğŸ”“ Site BaÅŸlÄ±ÄŸÄ±: {title}", flush=True)
            
            user_agent = sb.get_user_agent()
            sb_cookies = sb.get_cookies()
            for cookie in sb_cookies:
                cookies[cookie['name']] = cookie['value']
                
            print("   âœ… GiriÅŸ kartÄ± (Cookies) alÄ±ndÄ±!", flush=True)
            
        except Exception as e:
            print(f"   âŒ Selenium hatasÄ±: {e}", flush=True)
            
    return cookies, user_agent

def get_soup_fast(url, cookies, user_agent):
    """Curl_CFFI ile hÄ±zlÄ± istek atar (Chrome taklidi yaparak)."""
    headers = {
        'User-Agent': user_agent,
        'Referer': BASE_DOMAIN,
    }
    try:
        response = session.get(
            url, 
            cookies=cookies, 
            headers=headers, 
            impersonate="chrome110", 
            timeout=15
        )
        
        if response.status_code == 200:
            return BeautifulSoup(response.content, 'html.parser')
        elif response.status_code == 404:
            return "404"
        elif response.status_code == 403:
            # 403 durumunda Ã¶zel sinyal dÃ¶ndÃ¼r
            return "403"
    except Exception as e:
        print(f"   âš ï¸ HÄ±zlÄ± mod hatasÄ±: {e}", flush=True)
    return None

def get_video_source(soup):
    """Video kaynaÄŸÄ±nÄ± (iframe) bulur."""
    try:
        # 1. YÃ¶ntem: GÃ¼venli alan
        player_area = soup.find('div', class_='video-player-area')
        if player_area:
            iframe = player_area.find('iframe')
            if iframe: return iframe.get('src')
        
        # 2. YÃ¶ntem: Genel arama
        iframe = soup.find('iframe')
        if iframe and 'src' in iframe.attrs:
            return iframe['src']
            
        # 3. YÃ¶ntem: TÃ¼m iframeler
        iframes = soup.find_all('iframe')
        for frame in iframes:
            src = frame.get('src', '')
            if 'embed' in src or '.cfd' in src or 'player' in src:
                return src
    except: pass
    return ""

def get_full_movie_details(url, cookies, user_agent):
    """Film detaylarÄ±nÄ± Ã§eker. 403 alÄ±rsa '403' stringi dÃ¶ner."""
    soup = get_soup_fast(url, cookies, user_agent)
    
    # EÄŸer 403 aldÄ±ysak hemen bildir
    if soup == "403":
        return "403"

    # Standart boÅŸ ÅŸablon
    details = {
        "url": url,
        "videoUrl": "", 
        "description": "AÃ§Ä±klama bulunamadÄ±.", 
        "imdb": "0.0", 
        "genres": [], 
        "cast": [], 
        "year": "",
        "poster": "",
        "cover_image": "",
        "platform": "Platform DÄ±ÅŸÄ±", # VarsayÄ±lan deÄŸer
        "added_date": ""
    }
    
    if not soup or soup == "404": 
        return None

    try:
        # --- Metadata (Poster, Kapak vs) ---
        poster_div = soup.find('div', class_='poster')
        if poster_div and poster_div.find('img'):
            details['poster'] = poster_div.find('img').get('src')

        head_div = soup.find('div', id='head', class_='cover-image')
        if head_div and head_div.has_attr('style') and "url('" in head_div['style']:
            details['cover_image'] = head_div['style'].split("url('")[1].split("')")[0]

        # --- Video KaynaÄŸÄ± ---
        details["videoUrl"] = get_video_source(soup)

        # --- YENÄ° ALANLAR: Platform, Tarih, YÄ±l (SVG ile) ---
        
        # 1. Platform Bilgisi
        # Link iÃ§inde '/platform/' geÃ§en a etiketini arÄ±yoruz
        platform_link = soup.find('a', href=lambda x: x and '/platform/' in x)
        if platform_link:
            details['platform'] = platform_link.get_text(strip=True)
            
        # 2. Eklenme Tarihi (Upload.svg ikonu ile)
        # img src iÃ§inde 'Upload.svg' geÃ§en gÃ¶rseli bulup ebeveynindeki metni alÄ±yoruz
        upload_icon = soup.find('img', src=lambda x: x and 'Upload.svg' in x)
        if upload_icon:
            # parent genelde h6 veya div olur, text'i oradan alÄ±yoruz
            details['added_date'] = upload_icon.parent.get_text(strip=True)

        # 3. YapÄ±m YÄ±lÄ± (Calendar.svg ikonu ile - Daha Kesin)
        calendar_icon = soup.find('img', src=lambda x: x and 'Calendar.svg' in x)
        if calendar_icon:
            details['year'] = calendar_icon.parent.get_text(strip=True)

        # --- AÃ§Ä±klama ---
        summary_title = soup.find('h6', string=lambda t: t and 'Film Ã–zeti' in t)
        if summary_title:
            summary_p = summary_title.find_next('p')
            if summary_p: details["description"] = summary_p.get_text(strip=True)
        else:
            summ = soup.find('p', class_='summary-text')
            if summ: details["description"] = summ.get_text(strip=True)

        # --- Detay KutularÄ± (Eski YÃ¶ntem - Yedek) ---
        # EÄŸer yukarÄ±da Calendar.svg ile yÄ±l bulunamadÄ±ysa buradan da bakabilir
        info_boxes = soup.find_all('div', class_=lambda x: x and 'rounded-[10px]' in x and 'bg-white/[4%]' in x)
        
        for box in info_boxes:
            label_span = box.find('span', class_='text-xs')
            if label_span:
                label = label_span.get_text(strip=True)
                val_div = label_span.find_next_sibling('div') or label_span.find_next_sibling('h6')
                
                if val_div:
                    if "IMDB PuanÄ±" in label: 
                        details["imdb"] = val_div.get_text(strip=True)
                    elif "TÃ¼r" in label: 
                        details["genres"] = [a.get_text(strip=True) for a in val_div.find_all('a')]
                    elif "Oyuncular" in label: 
                        details["cast"] = [a.get_text(strip=True) for a in val_div.find_all('a')]
                    elif "YapÄ±m YÄ±lÄ±" in label and not details["year"]: 
                        # Sadece yukarÄ±daki SVG yÃ¶ntemi bulamadÄ±ysa buradan al
                        details["year"] = val_div.get_text(strip=True)

    except Exception as e: 
        print(f"   âš ï¸ Detay hatasÄ±: {e}", flush=True)
        pass
        
    return details

def main():
    print("ğŸ›¡ï¸ GÃ¼neÅŸ TV: Film Botu BaÅŸlatÄ±lÄ±yor (AkÄ±llÄ± GÃ¼ncelleme Modu)...", flush=True)

    # 1. ADIM: Selenium ile Ã‡erezleri Al
    cookies, user_agent = get_cookies_and_ua_with_selenium()
    
    if not cookies:
        print("âŒ Ã‡erezler alÄ±namadÄ±, Ã§Ä±kÄ±lÄ±yor.", flush=True)
        return

    # 2. ADIM: Mevcut Veriyi YÃ¼kle ve HÄ±zlÄ± Arama Seti OluÅŸtur
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                all_films = json.load(f)
            # URL'leri hÄ±zlÄ± kontrol iÃ§in bir kÃ¼meye (set) alÄ±yoruz
            existing_urls = {movie.get('url') for movie in all_films if 'url' in movie}
            print(f"ğŸ“¦ Mevcut veritabanÄ±: {len(all_films)} film yÃ¼klendi.", flush=True)
        except:
            all_films = []
            existing_urls = set()
    else:
        all_films = []
        existing_urls = set()

    page_num = 1
    empty_page_count = 0
    consecutive_existing_count = 0  # Arka arkaya kaÃ§ tane var olan film bulduk?

    while True:
        target_url = f"{BASE_DOMAIN}/filmler/page/{page_num}/"
        print(f"\n--- ğŸ“„ SAYFA {page_num} ANALÄ°ZÄ° (Kontrol: {consecutive_existing_count}/{CHECK_LIMIT}) ---", flush=True)
        
        soup = get_soup_fast(target_url, cookies, user_agent)
        
        # ANA SAYFADA 403 ALIRSAK
        if soup == "403":
            print("ğŸ”„ Sayfa eriÅŸiminde 403! Ã‡erez yenileniyor...", flush=True)
            cookies, user_agent = get_cookies_and_ua_with_selenium()
            soup = get_soup_fast(target_url, cookies, user_agent)

        if not soup or soup == "404":
            print("ğŸ Sayfa yok veya bitti.", flush=True)
            break

        # Filmleri Bul
        items = soup.find_all('div', class_='post-item')
        
        if not items:
            print("âš ï¸ Bu sayfada film bulunamadÄ±.", flush=True)
            empty_page_count += 1
            if empty_page_count >= 2: break
            page_num += 1
            continue

        empty_page_count = 0
        print(f"   ğŸ” {len(items)} film bulundu.", flush=True)

        for item in items:
            try:
                link_element = item.find('a')
                if not link_element: continue
                
                title = link_element.get('title', '').strip()
                movie_url = link_element.get('href', '')
                
                # --- AKILLI GÃœNCELLEME MANTIÄI ---
                if movie_url in existing_urls:
                    consecutive_existing_count += 1
                    print(f"   â­ï¸ Zaten mevcut: {title} (SayaÃ§: {consecutive_existing_count}/{CHECK_LIMIT})", flush=True)
                    
                    if consecutive_existing_count >= CHECK_LIMIT:
                        print(f"\nğŸ›‘ LÄ°MÄ°TE ULAÅILDI: Arka arkaya {CHECK_LIMIT} eski film bulundu.")
                        print("   GÃ¼ncel filmlerin hepsi tarandÄ±, iÅŸlem bitiriliyor.")
                        return  # ProgramÄ± tamamen durdur
                    
                    continue # Bir sonraki filme geÃ§
                else:
                    # Yeni bir film bulduk! SayacÄ± sÄ±fÄ±rla.
                    consecutive_existing_count = 0
                    print(f"   ğŸ†• Yeni FÄ°lm Tespit Edildi: {title}", flush=True)

                print(f"   â–¶ï¸ Analiz Ediliyor...", flush=True)
                
                # DetaylarÄ± Ã§ek
                meta = get_full_movie_details(movie_url, cookies, user_agent)
                
                # FÄ°LM DETAYINDA 403 ALIRSAK (HATA TELAFÄ°SÄ°)
                if meta == "403":
                    print("   ğŸš¨ FÄ°LM Ä°Ã‡Ä°NDE Ã‡EREZ BÄ°TTÄ°! Yenilenip tekrar deneniyor...", flush=True)
                    cookies, user_agent = get_cookies_and_ua_with_selenium()
                    # AynÄ± filmi tekrar dene
                    meta = get_full_movie_details(movie_url, cookies, user_agent)
                
                if meta and meta != "403":
                    meta['title'] = title # Listeden gelen baÅŸlÄ±ÄŸÄ± garantiye al
                    all_films.append(meta)
                    existing_urls.add(movie_url) # HÄ±zlÄ± listeye de ekle
                    
                    # AnlÄ±k KayÄ±t
                    with open(DATA_FILE, 'w', encoding='utf-8') as f:
                        json.dump(all_films, f, ensure_ascii=False, indent=2)
                    
                    print(f"   âœ… Eklendi: {title} | {meta['year']} | {meta['platform']}", flush=True)
                else:
                    print(f"   âŒ Veri alÄ±namadÄ±: {title}", flush=True)
                
            except Exception as e: 
                print(f"   âŒ Film iÅŸleme hatasÄ±: {e}", flush=True)
                continue

        page_num += 1

    print(f"\nğŸ‰ Ä°ÅŸlem tamamlandÄ±. Toplam veri: {len(all_films)}", flush=True)

if __name__ == "__main__":
    main()
